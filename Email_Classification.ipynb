{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19ab64c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParams\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac2e69f7",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "class Data_Preprocessing:\n",
    "    emails= pd.DataFrame()\n",
    "    \n",
    "    def __init__(self):\n",
    "        print('Object created......Data Preprocessing starts')\n",
    "        print('----------------------------------------------------------------')\n",
    "    \n",
    "    def read_data(self,input_dataset):\n",
    "        global emails\n",
    "        \n",
    "        print('Reading Data from the csv file')\n",
    "        emails= pd.read_csv(input_dataset, encoding='latin-1')\n",
    "        \n",
    "        print('Prints the first 5 rows of the dataframe')\n",
    "        print(emails.head())\n",
    "        print('----------------------------------------------------------------')\n",
    "        \n",
    "        print('Number of emails in each label')\n",
    "        print(emails.Label.value_counts())\n",
    "        print('----------------------------------------------------------------')\n",
    "        \n",
    "        print('A copy of the Email content is created')\n",
    "        text_feat= emails['Email'].copy()\n",
    "        print('----------------------------------------------------------------')\n",
    "        \n",
    "        print('Calling the text_process function to remove punctuation and stopwords.')\n",
    "        print('----------------------------------------------------------------')\n",
    "        print('This might take few minutes')\n",
    "        text_feat= text_feat.apply(self.text_process)\n",
    "        print('\\n')\n",
    "        print(text_feat.head())\n",
    "        \n",
    "        return text_feat\n",
    "\n",
    "    \n",
    "    def text_process(self, text):\n",
    "        #the text is translated by replacing empty string wth empty string and deleting all the characters found in string.punctuation\n",
    "        text= text.translate(str.maketrans('','',string.punctuation))\n",
    "        text = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n",
    "        return \" \".join(text)\n",
    "    \n",
    "    def stemmer(self, text):\n",
    "        #stemming of content\n",
    "        text = text.split()\n",
    "        words = \"\"\n",
    "        for i in text:\n",
    "                stemmer = SnowballStemmer(\"english\")\n",
    "                words += (stemmer.stem(i))+\" \"\n",
    "        return words\n",
    "        \n",
    "    def feature_creation(self, text_feat):\n",
    "        print('Initialize the TfIdfVectorizer')\n",
    "        vectorizer= TfidfVectorizer(stop_words='english')\n",
    "        features = vectorizer.fit_transform(text_feat)\n",
    "        print('***********Features created successfully*******************')\n",
    "        print('--------------------------------')\n",
    "        print('Features: ', features.shape)\n",
    "        print('\\n')\n",
    "        return features\n",
    "    \n",
    "    def featcreation_countvector(self, text_feat):\n",
    "        print('Initialize the count vector')\n",
    "        vector_count= CountVectorizer()\n",
    "        features_count= vector_count.fit_transform(text_feat)\n",
    "        print('Features using count vectorizer created successfully')\n",
    "        print('----------------------------------')\n",
    "        print('Features_count: ', features_count.shape)\n",
    "        print('\\n')\n",
    "        return features_count\n",
    "                \n",
    "    def split_train_test(self, features):\n",
    "        global emails\n",
    "        features_train, features_test, labels_train, labels_test = train_test_split(features, emails['Label'], test_size=0.3, random_state=111)\n",
    "        print('Features_train: ', features_train.shape)\n",
    "        print('Features_test: ', features_test.shape)\n",
    "        print('Labels_train: ', labels_train.shape)\n",
    "        print('Labels_test: ', labels_test.shape)\n",
    "        print('\\n')\n",
    "        return features_train, features_test, labels_train, labels_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb7e6bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process_one_email(self, text):\n",
    "        #the text is translated by replacing empty string wth empty string and deleting all the characters found in string.punctuation\n",
    "        text= text.translate(str.maketrans('','',string.punctuation))\n",
    "        text = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d292aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_dataset='./Datasets/final_dataset.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7912d7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object created......Data Preprocessing starts\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Creatig object of class Data_Preprocessing\n",
    "Processed_dataset= Data_Preprocessing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e858f93",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data from the csv file\n",
      "Prints the first 5 rows of the dataframe\n",
      "                                               Email   Label  Length\n",
      "0   cvs of candidates for rac support role  these...  NORMAL     126\n",
      "1   http : / / www . joelpittet . com  hello ,  i...    SPAM     940\n",
      "2   market internet access - no investment needed...    SPAM     294\n",
      "3    Greetings!!   I come to you with a sincere h...   FRAUD    2878\n",
      "4   the best possible mortgage  has your mortgage...    SPAM     568\n",
      "----------------------------------------------------------------\n",
      "Number of emails in each label\n",
      "NORMAL    1000\n",
      "SPAM      1000\n",
      "FRAUD     1000\n",
      "Name: Label, dtype: int64\n",
      "----------------------------------------------------------------\n",
      "A copy of the Email content is created\n",
      "----------------------------------------------------------------\n",
      "Calling the text_process function to remove punctuation and stopwords.\n",
      "----------------------------------------------------------------\n",
      "This might take few minutes\n",
      "\n",
      "\n",
      "0    cvs candidates rac support role three guys ava...\n",
      "1    http www joelpittet com hello visited www joel...\n",
      "2    market internet access investment needed marke...\n",
      "3    Greetings come sincere heart believing Almight...\n",
      "4    best possible mortgage mortgage search got fru...\n",
      "Name: Email, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Read data from the csv file, then removal of stopwords and punctuation\n",
    "text_feat= Processed_dataset.read_data(input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "685455e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(text):\n",
    "    #the text is translated by replacing empty string wth empty string and deleting all the characters found in string.punctuation\n",
    "    text= text.translate(str.maketrans('','',string.punctuation))\n",
    "    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "530596f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "vectorizer= TfidfVectorizer(stop_words='english')\n",
    "vectorizer.fit_transform(text_feat)\n",
    "\n",
    "abc_model = pickle.load(open('model/abc.pkl', 'rb'))\n",
    "bc_model = pickle.load(open('model/bc.pkl', 'rb'))\n",
    "dtc_model = pickle.load(open('model/dtc.pkl', 'rb'))\n",
    "etc_model = pickle.load(open('model/etc.pkl', 'rb'))\n",
    "knc_model = pickle.load(open('model/knc.pkl', 'rb'))\n",
    "mnb_model = pickle.load(open('model/mnb.pkl', 'rb'))\n",
    "rfc_model = pickle.load(open('model/rfc.pkl', 'rb'))\n",
    "svc_model = pickle.load(open('model/svc.pkl', 'rb'))\n",
    "lrc_model = pickle.load(open('model/lrc.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46984e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = input(\"Type anything\")\n",
    "X = text_process(X)\n",
    "Y =  vectorizer.transform([X])\n",
    "\n",
    "# AdaBoost Classifier\n",
    "predict_out_abc = abc_model.predict(Y)\n",
    "# Bagging Classifier\n",
    "predict_out_bc = bc_model.predict(Y)\n",
    "# Decision Tree Classifier\n",
    "predict_out_dtc = dtc_model.predict(Y)\n",
    "# ExtraTrees Classifier\n",
    "predict_out_etc = etc_model.predict(Y)\n",
    "# K-Nearest Neighbour\n",
    "predict_out_knc = knc_model.predict(Y)\n",
    "# Multinomial Naive Bayes\n",
    "predict_out_mnb = mnb_model.predict(Y)\n",
    "# Random Forest Classifier\n",
    "predict_out_rfc = rfc_model.predict(Y)\n",
    "# Support Vector Machine\n",
    "predict_out_svc = svc_model.predict(Y)\n",
    "# Logistic Regression\n",
    "predict_out_lrc = lrc_model.predict(Y)\n",
    "\n",
    "print(predict_out_abc)\n",
    "print(predict_out_bc)\n",
    "print(predict_out_dtc)\n",
    "print(predict_out_etc)\n",
    "print(predict_out_knc)\n",
    "print(predict_out_mnb)\n",
    "print(predict_out_rfc)\n",
    "print(predict_out_svc)\n",
    "print(predict_out_lrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6007528",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
